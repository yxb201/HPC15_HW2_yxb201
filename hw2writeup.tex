\documentclass[12pt]{article}

%% FONTS
%% To get the default sans serif font in latex, uncomment following line:
 \renewcommand*\familydefault{\sfdefault}
%%
%% to get Arial font as the sans serif font, uncomment following line:
%% \renewcommand{\sfdefault}{phv} % phv is the Arial font
%%
%% to get Helvetica font as the sans serif font, uncomment following line:
% \usepackage{helvet}
\usepackage[small,bf,up]{caption}
\renewcommand{\captionfont}{\footnotesize}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{graphics,epsfig,graphicx,float,subfigure,color}
\usepackage{amsmath,amssymb,amsbsy,amsfonts,amsthm}
\usepackage{url}
\usepackage{boxedminipage}
\usepackage[sf,bf,tiny]{titlesec}
 \usepackage[plainpages=false, colorlinks=true,
   citecolor=blue, filecolor=blue, linkcolor=blue,
   urlcolor=blue]{hyperref}
\usepackage{enumitem}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
% see documentation for titlesec package
% \titleformat{\section}{\large \sffamily \bfseries}
\titlelabel{\thetitle.\,\,\,}


\newcommand{\bs}{\boldsymbol}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\setlength{\emergencystretch}{20pt}

\begin{document}

\begin{center}

\large \textbf{%%
High Performance Computing \\ Assignment \#1  \\ Yuan-Xun Bao \\ yxb201@nyu.edu \quad N13392943}
\end{center}

% ****************************
\section{Github Repository}

\verb|https://github.com/yxb201/HPC15_HW1_yxb201.git|

\section{MPI Ring Communication}
For passing an integer in a ring, I tested my program on a 2 GHz Intel Core i7 Macbook Pro.

\noindent
Test run: \verb|mpirun -np 3 ./int_ring 2|

\begin{verbatim}
Loop 1: I am rank 1. I received 0 from rank 0. I sent 1 to rank 2.
Loop 1: I am rank 2. I received 1 from rank 1. I sent 3 to rank 0.
Loop 1: I am rank 0. I received 3 from rank 2. I sent 0 to rank 1.
Loop 2: I am rank 0. I received 6 from rank 2. I sent 3 to rank 1.
Loop 2: I am rank 1. I received 3 from rank 0. I sent 4 to rank 2.
Loop 2: I am rank 2. I received 4 from rank 1. I sent 6 to rank 0.
\end{verbatim}

\noindent
To test the latency on my system: \verb|mpirun -np 4 ./int_ring 10000000|

\begin{verbatim}
Total elapsed time is 18.677320 seconds.
Latency is 0.000000466934 seconds per send/receive.
\end{verbatim}

\noindent
To test the bandwidth of the network, I sent a 2MB array in a ring among 4 machines:

\noindent
\verb|mpirun -np 4 -hosts box527,box534,crunchy1,crunchy3 ./array_ring 10|
\begin{verbatim}
Total time elapsed is 0.831853 seconds.
Bandwidth of the network is 96.170874 MB/s.
\end{verbatim}

\section{Jacobi Smoother}
To test the strong scalability of my program, I run it with 2, 4, 8, 16, 32 processors on \verb|crunchy1|. For each case, I run my code for 5 times and compute the average computational time. 
\begin{verbatim}
mpirun -np 8 ./jacobi-mpi 10000000 10
\end{verbatim}

\noindent
The result is summarized in the following table:

\begin{center}
  \begin{tabular}{ | c | c | }
    \hline
    \# of procs & avg timing of 5 runs in seconds  \\ \hline
    2  & 0.8557976s  \\ \hline
    4  & 0.3914874s \\ \hline
    8  & 0.2454278s  \\ \hline
    16 & 0.201233s \\ \hline
    32 & 0.1416122s \\
    \hline
  \end{tabular}
\end{center}
It's much more difficult to parallelize Gauss-Seidal since $u_i^{(k+1)}$ needs values  $u_1^{(k+1)}, \dots, u_{i-1}^{(k+1)}$. Therefore, the processor to which $u_i^{(k+1)}$ belong has to wait until these values are available from other processors.


\end{document}
